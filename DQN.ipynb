{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucaliverani76/Chess_with_DQN/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbI5i-L0xond",
        "outputId": "f52c0cb3-0ce9-489d-b2e8-3b082e9f492b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/Chess\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Chess\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YbhOFTs6wXw",
        "outputId": "b820a18f-e4ef-4795-f16d-00e3ce57e578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Collecting chess\n",
            "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess\n",
            "Successfully installed chess-1.10.0\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "%run \"/content/drive/MyDrive/Colab Notebooks/Chess/Chessx.ipynb\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz76l1plyKBu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "import glob\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# sole purpose of this function is to upload data from previous Colab session\n",
        "def findoldcalcs(extn):\n",
        "    txtfiles = []\n",
        "\n",
        "    for file in glob.glob(extn):\n",
        "        txtfiles.append(file)\n",
        "\n",
        "    x=[]\n",
        "    for txt in txtfiles:\n",
        "        x=x+re.findall(r'\\d+', txt)\n",
        "    #print(x)\n",
        "    x=[int(y) for y in x]\n",
        "    if len(x)>0:\n",
        "        result=[max(x)]\n",
        "    else:\n",
        "        result=[]\n",
        "\n",
        "    if len(result)>0:\n",
        "        for n,y in enumerate(x):\n",
        "            #print(n,y)\n",
        "            if y!=max(x):\n",
        "              #print(txtfiles[n])\n",
        "              os.remove(txtfiles[n])\n",
        "        return int(result[0])\n",
        "    else:\n",
        "        return int(0)\n",
        "\n",
        "#this function looks if previous sessions of colab crushed and restart the system from there\n",
        "def Dogenericstuff(Q_action_value):\n",
        "    Savedweights_=findoldcalcs(\"*.pth\")\n",
        "    episode=np.floor(Savedweights_/1000)\n",
        "    minibatch_sample=Savedweights_-episode*1000\n",
        "\n",
        "    if episode!=0 or minibatch_sample!=0:\n",
        "        # loading weights\n",
        "        Q_action_value.wpath=weightfile+str(int(episode*1000+minibatch_sample))+'.pth'\n",
        "        #print(Q_target_value.wpath)\n",
        "        Q_action_value.loadWeights()\n",
        "        copyweightsfromto_Q_action_value_to_Q_target_value()\n",
        "\n",
        "    return episode, minibatch_sample\n",
        "\n",
        "\n",
        "def Savingweights():\n",
        "    # saving weights\n",
        "    Q_action_value.wpath=weightfile+str(int(episode*1000+minibatch_sample))+'.pth'\n",
        "    #print(Q_action_value.wpath)\n",
        "    Q_action_value.SaveWeights()\n",
        "\n",
        "\n",
        "def Defineorload_reply_memory(N):\n",
        "    n=findoldcalcs(\"*.pickle\")  # sole purpose of taking over where we left\n",
        "    print(n)\n",
        "\n",
        "    if n>0:\n",
        "      with open('Replaymemory'+str(n)+'.pickle', 'rb') as handle:\n",
        "          D = pickle.load(handle)\n",
        "    else:\n",
        "        D=collections.deque(maxlen=N)\n",
        "    return n,D\n",
        "\n",
        "def Save_reply_memory():\n",
        "    if (n)%1000==0:\n",
        "        print(\"Replay memory reached {} units\".format(n))\n",
        "        with open('Replaymemory'+str(n)+'.pickle', 'wb') as handle:\n",
        "            pickle.dump(D, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def ResetReplymemory():\n",
        "    txtfiles = []\n",
        "    for file in glob.glob(\"*.pickle\"):\n",
        "        txtfiles.append(file)\n",
        "    for f in txtfiles:\n",
        "        os.remove(f)\n",
        "\n",
        "def ResetWeight_History():\n",
        "    txtfiles = []\n",
        "    for file in glob.glob(\"*.pth\"):\n",
        "        txtfiles.append(file)\n",
        "    for f in txtfiles:\n",
        "        os.remove(f)\n",
        "\n",
        "def Trytogetoldweights():\n",
        "\n",
        "    try:\n",
        "      Q_action_value.wpath=weightfile +'.pthx'\n",
        "      Q_action_value.loadWeights()\n",
        "      copyweightsfromto_Q_action_value_to_Q_target_value()\n",
        "      print(\"loaded old dictionary\")\n",
        "\n",
        "    except:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKNJK7XGymLS",
        "outputId": "01d767d0-0119-4b44-bfee-c469201d405c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "loaded old dictionary\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Algorithm DQN\n",
        "# =============================================================================\n",
        "\n",
        "T=30 #number of action steps, or games moves\n",
        "\n",
        "\n",
        "epsilon=0.5 #greedy policy\n",
        "decay = 0.95\n",
        "min_epsilon = 0.2\n",
        "\n",
        "\n",
        "\n",
        " # initialize replay memory with size N\n",
        "N=10000\n",
        "\n",
        "lambda_=0.3 #discount factor\n",
        "\n",
        "N_minibatch_sampling=1000\n",
        "\n",
        "n_tests_on_batch=int(N/N_minibatch_sampling*1.5)\n",
        "\n",
        "N_episodes=5\n",
        "\n",
        "weightfile='model_weights_tanh_moreneurons_Hdiscountf'\n",
        "\n",
        "torch.set_printoptions(precision=10)\n",
        "\n",
        "\n",
        "initializeAgents()  #simply we initialize the weights of value functions\n",
        "\n",
        "\n",
        "Trytogetoldweights()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKjJY0-S4yzQ",
        "outputId": "8c693147-9753-48fb-c054-7048d310459f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode 0.0\n",
            "10000\n",
            "End replay memory\n",
            "Test io  minibatch 0.0\n",
            "Loss: 7.410744728986174e-08\n",
            "Test io  minibatch 1.0\n",
            "Loss: 6.125810614321381e-08\n",
            "Test io  minibatch 2.0\n",
            "Loss: 5.202661412795351e-08\n",
            "Test io  minibatch 3.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "episode, minibatch_sample= Dogenericstuff(Q_action_value)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while episode < N_episodes:\n",
        "\n",
        "    print(\"episode {0}\".format(episode))\n",
        "\n",
        "    # I fill the Replay memory\n",
        "    n,D=Defineorload_reply_memory(N)\n",
        "\n",
        "\n",
        "    while n<N:\n",
        "\n",
        "        #sequences can start from always the initial\n",
        "        #state or in a random state, depending how we build the fuction initialize_sequence()\n",
        "\n",
        "        S=initialize_sequences()\n",
        "        white=True\n",
        "\n",
        "\n",
        "\n",
        "        for j in range(T):\n",
        "\n",
        "            possible_actions=PossibleActions(S)\n",
        "\n",
        "            if (possible_actions==[] or possible_actions==None):\n",
        "                break\n",
        "\n",
        "            if random.uniform(0, 1)<epsilon:\n",
        "                #especially at the beginning where we have no experince we tend to choose random actions\n",
        "                action=possible_actions[random.randrange(len(possible_actions))]\n",
        "            else:\n",
        "                #we evaluate the best possible action given the current state\n",
        "                action_values =eval_Q_action_value(S,possible_actions)\n",
        "\n",
        "                if white:\n",
        "                    i=np.argmax(action_values)\n",
        "                else:\n",
        "                    i=np.argmin(action_values)\n",
        "\n",
        "                action=possible_actions[i]\n",
        "\n",
        "            S_next=Implement_action(S,action)\n",
        "\n",
        "            reward=Reward(S)\n",
        "\n",
        "            if white:\n",
        "                D.append([S,action,reward,S_next])\n",
        "                n=n+1\n",
        "                Save_reply_memory()\n",
        "\n",
        "\n",
        "\n",
        "            S=S_next\n",
        "\n",
        "\n",
        "            white=not white\n",
        "\n",
        "            if S.loc[0,\"isover\"]:\n",
        "                break;\n",
        "\n",
        "\n",
        "\n",
        "    print(\"End replay memory\")\n",
        "        # current_replay_memory_ratio=int(np.ceil(len(D)/N_minibatch_sampling))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    while minibatch_sample < n_tests_on_batch:\n",
        "\n",
        "        print(\"Test io  minibatch {0}\".format(minibatch_sample))\n",
        "\n",
        "        S_s,action_s,reward_s,S_next_s =extract_minibatch(D, N_minibatch_sampling)  #extract data from batch\n",
        "\n",
        "        y = (reward_s+ lambda_* Max_Q_target_value(S_next_s)).astype(np.float32)\n",
        "\n",
        "        # trying to optimize the fit between Q_action_value and y\n",
        "        R=OptimizeQ_action_value(S_s,y)\n",
        "\n",
        "        Savingweights()\n",
        "\n",
        "        minibatch_sample=minibatch_sample+1\n",
        "\n",
        "\n",
        "    minibatch_sample=0\n",
        "\n",
        "    #at the end of every training we align the target and action value function\n",
        "    # hoping that these asyntotic optimization will converge on something meaningful\n",
        "    copyweightsfromto_Q_action_value_to_Q_target_value()\n",
        "\n",
        "    # I start reducing the random moves and I follow more\n",
        "    #and more the Q function advises\n",
        "    epsilon = max(min_epsilon, epsilon*decay)\n",
        "\n",
        "    n=0\n",
        "    ResetReplymemory()\n",
        "\n",
        "\n",
        "    episode=episode+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#And finally\n",
        "ResetWeight_History()\n",
        "Q_action_value.wpath=weightfile+'.pthx'\n",
        "Q_action_value.SaveWeights()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Pb6bbt0-dGCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPgWS32wMyK9"
      },
      "outputs": [],
      "source": [
        "R1=R.clone().detach().cpu().numpy()\n",
        "output_np = R1\n",
        "# Plot input and output\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "\n",
        "plt.plot(output_np, label='NN')\n",
        "\n",
        "plt.plot(y+0.00001, label='Rewards')\n",
        "plt.title('Input Data')\n",
        "plt.legend()\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1Gissgy-n-cJ1lPaGpHcNDRfz5s8027U_",
      "authorship_tag": "ABX9TyOEjdAq2IsLFUALuxZ2hsMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}