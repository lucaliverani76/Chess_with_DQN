{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNP3ZUOHO7G8xGpELJ2jxcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucaliverani76/Chess_with_DQN/blob/main/Chessx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  !pip install chess\n",
        "else:\n",
        "  print('Not running on CoLab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mLssbizSV4T",
        "outputId": "048d80b6-3c46-41b6-b4b7-9a3e019697ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n",
            "Collecting chess\n",
            "  Downloading chess-1.10.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chess\n",
            "Successfully installed chess-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpfktahuRFdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc78ee3-2bcc-401d-a9bc-08f9060c0cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Created on Sat Oct 14 19:26:02 2023\n",
        "\n",
        "@author: luca\n",
        "\"\"\"\n",
        "\n",
        "import chess\n",
        "import chess.svg\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Here I simply gives some additional scores to some situations to add to the value function\n",
        "CHECKMATE=20\n",
        "SIMPLECHECK=6\n",
        "ADVANTAGE_INSUFFICIENT_MATERIAL=6\n",
        "\n",
        "TOTAL_COLOR_SCORE=37 * 2 #this is just the sum of the value assigned to every piece by the chess library\n",
        "# array([[ 4.,  2.,  3.,  5.,  6.,  3.,  2.,  4.],\n",
        "#        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
        "#        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "#        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "#        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "#        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "#        [-1., -1., -1., -1., -1., -1., -1., -1.],\n",
        "#        [-4., -2., -3., -5., -6., -3., -2., -4.]], dtype=float32)\n",
        "\n",
        "\n",
        "\n",
        "DATAINPUT_SIZE=8*8 + 4\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def PossibleActions(board_state):\n",
        "    board = chess.Board()\n",
        "    board.set_fen(board_state.board[0])\n",
        "    move1=board.legal_moves\n",
        "    listofmoves=[str(move) for move in move1]\n",
        "    return listofmoves\n",
        "\n",
        "def drawboard(board_state):\n",
        "    board = chess.Board()\n",
        "    board.set_fen(board_state.board[0])\n",
        "    print(board)\n",
        "\n",
        "# =============================================================================\n",
        "# Thanks niklas\n",
        "# https://github.com/niklasf/python-chess/issues/404\n",
        "#\n",
        "# =============================================================================\n",
        "def convert_to_num(board):\n",
        "        l = [None] * 64\n",
        "        for sq in chess.scan_reversed(board.occupied_co[chess.WHITE]):  # Check if white\n",
        "            l[sq] = board.piece_type_at(sq)\n",
        "        for sq in chess.scan_reversed(board.occupied_co[chess.BLACK]):  # Check if black\n",
        "            l[sq] = -board.piece_type_at(sq)\n",
        "        new_list = [0 if x is None else x for x in l]\n",
        "        temp=  [np.array(new_list).reshape(-1,8).astype(np.float32)]\n",
        "        return temp\n",
        "\n",
        "\n",
        "\n",
        "def initialize_sequences(*args):\n",
        "\n",
        "    if len(args) == 1 and isinstance(args[0], np.ndarray):\n",
        "        board_state=pd.DataFrame(columns=[\"board\",\"board_np\",\"im\",\"ckm\",\"ck\",\"isover\", \"score\"])\n",
        "        board_state.loc[0] = [args[0][0],args[0][1],args[0][2],args[0][3],args[0][4],args[0][5], args[0][6]]\n",
        "\n",
        "    if len(args)==0:\n",
        "        board = chess.Board()\n",
        "        board_state=pd.DataFrame(columns=[\"board\",\"board_np\",\"im\",\"ckm\",\"ck\",\"isover\", \"score\"])\n",
        "        board_state.loc[0] = [board.fen(),convert_to_num(board),\n",
        "                              float(board.is_insufficient_material()),float(board.is_checkmate()),float(board.is_check())\\\n",
        "                                  ,float(board.is_insufficient_material()  or board.is_checkmate()),0 ]\n",
        "        board_state.loc[0, \"score\"]= Reward(board_state)\n",
        "    return board_state\n",
        "\n",
        "\n",
        "def fromChesstoS(board):\n",
        "    board_state=pd.DataFrame(columns=[\"board\",\"board_np\",\"im\",\"ckm\",\"ck\",\"isover\",\"score\"])\n",
        "    board_state.loc[0] = [board.fen(),convert_to_num(board),\n",
        "                          float(board.is_insufficient_material()),float(board.is_checkmate()),float(board.is_check())\\\n",
        "                              ,float(board.is_insufficient_material()  or board.is_checkmate()), 0]\n",
        "    board_state.loc[0, \"score\"]= Reward(board_state)\n",
        "\n",
        "    return board_state\n",
        "\n",
        "\n",
        "def Implement_action(board_state,Action):\n",
        "    B_result=initialize_sequences()\n",
        "    board = chess.Board()\n",
        "    board_state_fen=board_state.board[0]\n",
        "    board.set_fen(board_state_fen)\n",
        "    board.push_san(Action)\n",
        "    B_result.loc[0,\"board_np\"]=convert_to_num(board)\n",
        "    B_result.loc[0,\"im\"]=float(board.is_insufficient_material())\n",
        "    B_result.loc[0,\"ckm\"]=float(board.is_checkmate())\n",
        "    B_result.loc[0,\"ck\"]=float(board.is_check())\n",
        "    B_result.loc[0,\"isover\"]=float(board.is_insufficient_material()  or board.is_checkmate())\n",
        "    B_result.loc[0,\"board\"]=board.fen()\n",
        "    board_state.loc[0, \"score\"]= Reward(B_result)\n",
        "    return B_result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Copy_board_state(board_state,board_state_source):\n",
        "    L=board_state_source.values.tolist()[0].copy()\n",
        "    # print((board_state.loc[0,\"board_np\"]).shape)\n",
        "    board_state.loc[0,\"board\"]=L[0]\n",
        "    # print((L[1].copy()).shape)\n",
        "    board_state.loc[0,\"board_np\"]=L[1]\n",
        "    board_state.loc[0,\"im\"]=L[2]\n",
        "    board_state.loc[0,\"ckm\"]=L[3]\n",
        "    board_state.loc[0,\"ck\"]=L[4]\n",
        "    board_state.loc[0,\"isover\"]=L[5]\n",
        "    board_state.loc[0, \"score\"]= L[6]\n",
        "\n",
        "\n",
        "\n",
        "def Reward(board_state):\n",
        "\n",
        "    board_np=board_state.loc[0,\"board_np\"]\n",
        "    # print(board_np)\n",
        "    board_np=np.array(board_np).astype(int)\n",
        "\n",
        "    reward=np.sum(board_np).astype(np.float32)/np.sum(np.abs(board_np.astype(np.float32)))\n",
        "    board_state.loc[0,\"ck\"] *SIMPLECHECK/np.sum(np.abs(board_np.astype(np.float32))) \\\n",
        "       + board_state.loc[0,\"im\"]*ADVANTAGE_INSUFFICIENT_MATERIAL/np.sum(np.abs(board_np.astype(np.float32)))\\\n",
        "           + board_state.loc[0,\"ckm\"]*ADVANTAGE_INSUFFICIENT_MATERIAL/np.sum(np.abs(board_np.astype(np.float32)))\n",
        "\n",
        "    return reward/100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def define_parameters():\n",
        "    params = dict()\n",
        "\n",
        "    params['learning_rate'] = 0.001\n",
        "\n",
        "    params['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    params[\"weight_poth\"]='model_weights.pth'\n",
        "\n",
        "    return (params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DQNAgent(torch.nn.Module):\n",
        "    def __init__(self, params):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.learning_rate = params['learning_rate']\n",
        "\n",
        "        self.first_layer = 200\n",
        "        self.second_layer = 100\n",
        "        self.third_layer = 50\n",
        "\n",
        "        self.wpath=params[\"weight_poth\"]\n",
        "\n",
        "        self.optimizer = None\n",
        "        self.network()\n",
        "\n",
        "    def network(self):\n",
        "        # Layers\n",
        "        self.f1 = nn.Linear(DATAINPUT_SIZE, self.first_layer)\n",
        "        self.f2 = nn.Linear(self.first_layer, self.second_layer)\n",
        "        self.f3 = nn.Linear(self.second_layer, self.third_layer)\n",
        "        self.f4 = nn.Linear(self.third_layer, 1)\n",
        "\n",
        "    def loadWeights(self):\n",
        "        self.load_state_dict(torch.load(self.wpath,map_location=torch.device(params['device'])))\n",
        "\n",
        "\n",
        "    def SaveWeights(self):\n",
        "        torch.save(self.state_dict(), self.wpath)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.sigmoid(self.f1(x))\n",
        "        x = F.relu(self.f2(x))\n",
        "        x = F.relu(self.f3(x))\n",
        "        x = self.f4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DQNAgent_tanh(DQNAgent):\n",
        "    def __init__(self, params):\n",
        "        super().__init__(params)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.tanh(self.f1(x))\n",
        "        x = F.relu(self.f2(x))\n",
        "        x = F.relu(self.f3(x))\n",
        "        x = self.f4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DQNAgent_morebrain(DQNAgent):\n",
        "    def __init__(self, params):\n",
        "        super().__init__(params)\n",
        "        self.first_layer = 300\n",
        "        self.second_layer = 200\n",
        "        self.third_layer = 50\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.elu(self.f1(x))\n",
        "        # print(x)\n",
        "        x = F.elu(self.f2(x))\n",
        "        x = F.elu(self.f3(x))\n",
        "        x = self.f4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "params=define_parameters()\n",
        "\n",
        "Q_action_value=DQNAgent_morebrain(params).to(params['device'])\n",
        "Q_target_value=DQNAgent_morebrain(params).to(params['device'])\n",
        "\n",
        "\n",
        "def initializeAgents():\n",
        "    for layer_ in Q_action_value.children():\n",
        "        # torch.nn.init.uniform_(layer_.weight,-10.0, 10.0).to(params[\"device\"])\n",
        "        torch.nn.init.xavier_normal_(layer_.weight).to(params[\"device\"])\n",
        "        # nn.init.zeros_(layer_.bias).to(params[\"device\"])\n",
        "        nn.init.constant_(layer_.bias, -0.5).to(params[\"device\"])\n",
        "\n",
        "\n",
        "    Q_target_value.load_state_dict(Q_action_value.state_dict())\n",
        "    for param in Q_target_value.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "def copyweightsfromto_Q_action_value_to_Q_target_value():\n",
        "    Q_target_value.load_state_dict(Q_action_value.state_dict())\n",
        "\n",
        "\n",
        "\n",
        "def eval_Q_action_value(S,possible_actions):\n",
        "    if (possible_actions==[] or possible_actions==None):\n",
        "        return [0]\n",
        "    torch.set_grad_enabled(False)\n",
        "    action_list=[]\n",
        "    for action in possible_actions:\n",
        "        Snx=Implement_action(S,action)\n",
        "        v=np.hstack((Snx.board_np[0][0].copy().reshape(1,-1).squeeze(),np.array([Snx.im[0],Snx.ckm[0],Snx.ck[0], Snx.score[0]])))\n",
        "        v=v.astype(np.float32)\n",
        "        action_list.append(v)\n",
        "    AL=np.array(action_list)\n",
        "    action_values=(Q_action_value.forward(torch.tensor(AL, device=params['device'])).cpu().numpy()[0])\n",
        "    return action_values\n",
        "\n",
        "\n",
        "def eval_Q_target_value(S,possible_actions):\n",
        "    if (possible_actions==[] or possible_actions==None):\n",
        "        return [0]\n",
        "    torch.set_grad_enabled(False)\n",
        "    action_list=[]\n",
        "    for action in possible_actions:\n",
        "        Snx=Implement_action(S,action)\n",
        "        v=np.hstack((Snx.board_np[0][0].copy().reshape(1,-1).squeeze(),np.array([Snx.im[0],Snx.ckm[0],Snx.ck[0],Snx.score[0]])))\n",
        "        v=v.astype(np.float32)\n",
        "        action_list.append(v)\n",
        "    AL=np.array(action_list)\n",
        "    action_values=(Q_target_value.forward(torch.tensor(AL, device=params['device'])).cpu().numpy()[0])\n",
        "    return action_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_minibatch(D, length_batch):\n",
        "    array_=np.array(random.sample(D, length_batch),     dtype=object)\n",
        "    # columns_tuple = tuple(map(np.array,zip(*array_)))\n",
        "\n",
        "    return array_[:,0], array_[:,1], array_[:,2], array_[:,3]\n",
        "\n",
        "\n",
        "def Max_Q_target_value(Ss):\n",
        "\n",
        "    \"\"\"Max_Q_t_values=[]\n",
        "\n",
        "    for S in Ss:\n",
        "\n",
        "        #black moves\n",
        "        possible_actions=PossibleActions(S)\n",
        "        if (possible_actions==[] or possible_actions==None):\n",
        "            Max_Q_t_values.append(0)\n",
        "            continue\n",
        "        #we evaluate the best possible action given the current state for black\n",
        "        target_values =eval_Q_target_value(S,possible_actions)\n",
        "        # i=np.argmin(target_values)\n",
        "\n",
        "\n",
        "        # B_next=Implement_action(S,possible_actions[i])\n",
        "\n",
        "        # #white moves\n",
        "\n",
        "        # possible_actions=PossibleActions(B_next)\n",
        "        # if (possible_actions==[] or possible_actions==None):\n",
        "        #     Max_Q_t_values.append(0)\n",
        "        #     continue\n",
        "        # #we evaluate the best possible action given the current state for black\n",
        "        # target_values =eval_Q_target_value(B_next,possible_actions)\n",
        "        i=np.argmax(target_values)\n",
        "\n",
        "        Max_Q_t_values.append(target_values[i])\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    Max_Q_t_values=[]\n",
        "\n",
        "    for S in Ss:\n",
        "\n",
        "\n",
        "        possible_actions=PossibleActions(S)\n",
        "        if (possible_actions==[] or possible_actions==None):\n",
        "            Max_Q_t_values.append(0)\n",
        "            continue\n",
        "\n",
        "\n",
        "        Nextmoves_values=[]\n",
        "        for i in range (len(possible_actions)):\n",
        "\n",
        "\n",
        "            B_next=Implement_action(S,possible_actions[i])\n",
        "\n",
        "\n",
        "            possible_actions_next=PossibleActions(B_next)\n",
        "            if (possible_actions_next==[] or possible_actions_next==None):\n",
        "                Nextmoves_values.append(0)\n",
        "                continue\n",
        "\n",
        "            target_values =eval_Q_target_value(B_next,possible_actions_next)\n",
        "            i=np.argmin(target_values)\n",
        "\n",
        "            Nextmoves_values.append(target_values[i])\n",
        "\n",
        "        Max_Q_t_values.append(max(Nextmoves_values))\n",
        "\n",
        "\n",
        "    return np.array(Max_Q_t_values)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def OptimizeQ_action_value(S_s,y):\n",
        "    agent=Q_action_value\n",
        "    agent.optimizer=optim.Adam(agent.parameters(), weight_decay=0, lr=params['learning_rate'])\n",
        "    target_f=torch.tensor(y.reshape(-1,1), device=params['device']).float()\n",
        "    # print(target_f)\n",
        "\n",
        "\n",
        "    R=[]\n",
        "    for Snx in S_s:\n",
        "        v=np.hstack((Snx.board_np[0][0].copy().reshape(1,-1).squeeze(),np.array([Snx.im[0],Snx.ckm[0],Snx.ck[0], Snx.score[0]])))\n",
        "        v=v.astype(np.float32)\n",
        "        R.append(v)\n",
        "    x_data=torch.tensor(np.array(R),device=params['device']).float()\n",
        "\n",
        "\n",
        "    agent.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    num_epochs = 1000\n",
        "    for epoch in range(num_epochs):\n",
        "        output=agent.forward(x_data)\n",
        "\n",
        "        agent.optimizer.zero_grad()\n",
        "        loss = F.mse_loss(output, target_f)\n",
        "        loss.backward()\n",
        "        agent.optimizer.step()\n",
        "\n",
        "        # Print the loss every 100 epochs\n",
        "    print(f'Loss: {loss.item()}')\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_Q_value_generic(Q,S,possible_actions):\n",
        "    if (possible_actions==[] or possible_actions==None):\n",
        "        return [0]\n",
        "    torch.set_grad_enabled(False)\n",
        "    action_list=[]\n",
        "    for action in possible_actions:\n",
        "        Snx=Implement_action(S,action)\n",
        "        v=np.hstack((Snx.board_np[0][0].copy().reshape(1,-1).squeeze(),np.array([Snx.im[0],Snx.ckm[0],Snx.ck[0], Snx.score[0]])))\n",
        "        v=v.astype(np.float32)\n",
        "        action_list.append(v)\n",
        "    AL=np.array(action_list)\n",
        "    action_values=(Q.forward(torch.tensor(AL, device=params['device'])).cpu().numpy()[0])\n",
        "    return action_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "#\n",
        "# °°°°°°°°°°°°°°°°°°°°°°\n",
        "#\n",
        "# SIMPLE MECHANICAL TURK\n",
        "#\n",
        "# °°°°°°°°°°°°°°°°°°°°°°\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def basefunction_mt(S,n,R,L,choice):\n",
        "    possible_actions=PossibleActions(S)\n",
        "\n",
        "\n",
        "    if (possible_actions==[] or possible_actions==None) or n==0:\n",
        "        R1=R\n",
        "        L.append([R,choice])\n",
        "        return 0\n",
        "\n",
        "    for ch,a in enumerate(possible_actions):\n",
        "        S_next=Implement_action(S,a)\n",
        "        if n==1:\n",
        "\n",
        "            basefunction_mt(S_next,n-1,Reward(S_next),L,choice)\n",
        "\n",
        "        if n==2:\n",
        "            basefunction_mt(S_next,n-1,Reward(S_next),L,ch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mechanical_turk(S):\n",
        "    possible_actions=PossibleActions(S)\n",
        "    P=[]\n",
        "    if (possible_actions==[] or possible_actions==None):\n",
        "        return []\n",
        "    Mo_ves=[]\n",
        "    Lv=2\n",
        "    basefunction_mt(S,Lv,0,Mo_ves,0)\n",
        "    Mo_ves=np.array(Mo_ves)\n",
        "    # print(Mo_ves)\n",
        "    for k in range(len(possible_actions)):\n",
        "        W=np.where(Mo_ves[:,1]==k)\n",
        "        P.append(np.amin(Mo_ves[W[0],0]))\n",
        "    P=np.array(P)\n",
        "    return P\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ________________________________________________________\n",
        "# =============================================================================\n"
      ],
      "metadata": {
        "id": "pJPkGDsZ5bCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}